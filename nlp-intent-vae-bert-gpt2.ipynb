{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1.数据准备"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["# from datasets import load_dataset\n","\n","# dataset = load_dataset(\"imdb\")\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel\n","from torch.distributions import MultivariateNormal, Categorical\n","from torch.utils.data import Dataset, DataLoader\n","\n","## Load data\n","df_b = pd.read_csv('/kaggle/input/intent-dataset/data/banking/train.tsv', sep='\\t')\n","df_c = pd.read_csv('/kaggle/input/intent-dataset/data/clinc/train.tsv', sep='\\t')\n","df_b_te = pd.read_csv('/kaggle/input/intent-dataset/data/banking/test.tsv', sep='\\t')\n","df_b_dev = pd.read_csv('/kaggle/input/intent-dataset/data/banking/dev.tsv', sep='\\t')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T08:27:46.774041Z","iopub.status.busy":"2024-05-30T08:27:46.773158Z","iopub.status.idle":"2024-05-30T08:27:46.785743Z","shell.execute_reply":"2024-05-30T08:27:46.784515Z","shell.execute_reply.started":"2024-05-30T08:27:46.774002Z"},"trusted":true},"outputs":[],"source":["df = df_b\n","df_te = df_b_te\n","num_classes = df['label'].nunique()#*2"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T08:27:48.456542Z","iopub.status.busy":"2024-05-30T08:27:48.456117Z","iopub.status.idle":"2024-05-30T08:27:48.473329Z","shell.execute_reply":"2024-05-30T08:27:48.472337Z","shell.execute_reply.started":"2024-05-30T08:27:48.456508Z"},"trusted":true},"outputs":[],"source":["label_mapping = {v:i for i,v in enumerate(df['label'].unique())}\n","df['label_num'] = df['label'].map(label_mapping)\n","df_te['label_num'] = df_te['label'].map(label_mapping)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T08:36:23.471879Z","iopub.status.busy":"2024-05-30T08:36:23.471523Z","iopub.status.idle":"2024-05-30T08:36:23.620536Z","shell.execute_reply":"2024-05-30T08:36:23.619632Z","shell.execute_reply.started":"2024-05-30T08:36:23.471853Z"},"trusted":true},"outputs":[],"source":["import torch.nn.functional as F\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, dataframe, num_classes=20):\n","        self.data = dataframe\n","        self.num_classes = num_classes\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        text = self.data.loc[index, 'text']\n","#         encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True, max_length=128, padding='max_length')\n","#         label = self.data.loc[index, 'label_num']\n","#         one_hot_label = F.one_hot(torch.tensor(label), num_classes=self.num_classes)\n","#         inputs_ids = encoded_input['input_ids'].squeeze(0)\n","#         attention_mask = encoded_input['attention_mask'].squeeze(0)\n","#         return inputs_ids, attention_mask, one_hot_label\n","        return {'text': text}"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T08:37:03.007769Z","iopub.status.busy":"2024-05-30T08:37:03.007411Z","iopub.status.idle":"2024-05-30T08:37:03.012199Z","shell.execute_reply":"2024-05-30T08:37:03.011245Z","shell.execute_reply.started":"2024-05-30T08:37:03.007739Z"},"trusted":true},"outputs":[],"source":["dataset = CustomDataset(df, num_classes)\n","val_dataset = CustomDataset(df_te, num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["# 2.模型定义"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T08:29:05.971127Z","iopub.status.busy":"2024-05-30T08:29:05.970269Z","iopub.status.idle":"2024-05-30T08:29:05.975092Z","shell.execute_reply":"2024-05-30T08:29:05.974084Z","shell.execute_reply.started":"2024-05-30T08:29:05.971095Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    batch_size = 12\n","    ngpu = 2"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T08:29:08.201602Z","iopub.status.busy":"2024-05-30T08:29:08.201249Z","iopub.status.idle":"2024-05-30T08:29:15.346104Z","shell.execute_reply":"2024-05-30T08:29:15.345126Z","shell.execute_reply.started":"2024-05-30T08:29:08.201573Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66bb993d199248f88009305375c97505","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d02e70e40b7427ba185b52818d6ba0c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55417498b24a4028a3b3d255eac67ac4","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8dc92949409434c8c396d6677d42f8d","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51baafd71c5a4b70b42ab709052f44f7","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebcaa1a58af548d4bdde8c42ab773cc7","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c47a205fe7ff4dff8e83a2879fb3acd1","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b86e040e8b646279c3e5f9228b76a15","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# 加载BERT和GPT-2\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","\n","gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# 保证GPT-2的词汇表与BERT的一致\n","gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n","\n","class VAE(nn.Module):\n","    def __init__(self, bert_model, gpt2_model):\n","        super(VAE, self).__init__()\n","        self.bert_encoder = bert_model\n","        self.latent_dim = bert_model.config.hidden_size\n","        \n","        self.fc_mu = nn.Linear(self.latent_dim, self.latent_dim)\n","        self.fc_logvar = nn.Linear(self.latent_dim, self.latent_dim)\n","        self.bn_mu = nn.BatchNorm1d(self.latent_dim)  # 添加批量归一化层\n","#         self.bn_var = nn.BatchNorm1d(self.latent_dim)  # 添加批量归一化层\n","        \n","        self.gpt2_decoder = gpt2_model\n","        self.gpt2_config = gpt2_model.config\n","        \n","    def encode(self, input_ids, attention_mask=None):\n","        outputs = self.bert_encoder(input_ids, attention_mask=attention_mask)\n","        hidden_state = outputs.last_hidden_state[:, 0, :]  # 使用[CLS] token表示\n","        mu = self.bn_mu(self.fc_mu(hidden_state))\n","        logvar = self.fc_logvar(hidden_state)\n","        return mu, logvar\n","    \n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","    \n","    def kl_divergence(self, z, z_l):\n","        q_z = MultivariateNormal(z, torch.eye(z.shape[1]).cuda())\n","        p_z_components = MultivariateNormal(z_l, torch.eye(z_l.shape[1]).cuda())\n","        log_q_z = q_z.log_prob(z)\n","        ## 当L=1\n","        log_p_z = p_z_components.log_prob(z_l)\n","        return torch.mean(log_q_z - log_p_z)\n","    \n","    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask=None):\n","        mu, logvar = self.encode(input_ids, attention_mask)\n","        z = self.reparameterize(mu, logvar)\n","        z_l = self.reparameterize(mu, logvar)\n","        kl_div = self.kl_divergence(z, z_l)\n","        # 将latent vector重复多次以匹配decoder_input_ids的序列长度\n","        latent_hidden = z.unsqueeze(1).repeat(1, decoder_input_ids.size(1), 1)\n","        # 将labels参数设置为与input_ids相同，因为再语言建模任务中，模型的目标是预测下一个单词，因此目标标签通常是输入数据的右移版本。\n","        gpt2_outputs = self.gpt2_decoder(inputs_embeds=latent_hidden, attention_mask=decoder_attention_mask, labels=decoder_input_ids)\n","        return gpt2_outputs.logits, kl_div\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# 初始化模型\n","model = VAE(bert_model, gpt2_model).to(device)\n","\n","# model = nn.DataParallel(model, device_ids=list(range(CFG.ngpu)))"]},{"cell_type":"markdown","metadata":{},"source":["# 3.损失函数和优化器"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T08:29:32.722852Z","iopub.status.busy":"2024-05-30T08:29:32.722473Z","iopub.status.idle":"2024-05-30T08:29:32.733251Z","shell.execute_reply":"2024-05-30T08:29:32.730861Z","shell.execute_reply.started":"2024-05-30T08:29:32.722796Z"},"trusted":true},"outputs":[],"source":["from torch.optim import Adam\n","\n","# def vae_loss_function(recon_logits, target_ids, mu, logvar):\n","#     recon_loss = nn.CrossEntropyLoss()(recon_logits.view(-1, recon_logits.size(-1)), target_ids.view(-1))\n","#     kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","#     return recon_loss + kld_loss\n","\n","def vae_loss_function(recon_logits, target_ids, kl_div):\n","    recon_loss = nn.CrossEntropyLoss()(recon_logits.view(-1, recon_logits.size(-1)), target_ids.view(-1))\n","    return recon_loss + kl_div\n","\n","\n","optimizer = Adam(model.parameters(), lr=1e-4)"]},{"cell_type":"markdown","metadata":{},"source":["# 4.训练模型"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T08:37:14.912468Z","iopub.status.busy":"2024-05-30T08:37:14.912130Z","iopub.status.idle":"2024-05-30T08:46:54.331187Z","shell.execute_reply":"2024-05-30T08:46:54.330154Z","shell.execute_reply.started":"2024-05-30T08:37:14.912444Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trian_loader length: 2251\n","Epoch 0: [0/2251], Loss: 7.99\n","Epoch 0: [50/2251], Loss: 5.15\n","Epoch 0: [100/2251], Loss: 4.87\n","Epoch 0: [150/2251], Loss: 4.70\n","Epoch 0: [200/2251], Loss: 4.62\n","Epoch 0: [250/2251], Loss: 4.55\n","Epoch 0: [300/2251], Loss: 4.51\n","Epoch 0: [350/2251], Loss: 4.47\n","Epoch 0: [400/2251], Loss: 4.43\n","Epoch 0: [450/2251], Loss: 4.42\n","Epoch 0: [500/2251], Loss: 4.40\n","Epoch 0: [550/2251], Loss: 4.34\n","Epoch 0: [600/2251], Loss: 4.31\n","Epoch 0: [650/2251], Loss: 4.28\n","Epoch 0: [700/2251], Loss: 4.23\n","Epoch 0: [750/2251], Loss: 4.19\n","Epoch 0: [800/2251], Loss: 4.17\n","Epoch 0: [850/2251], Loss: 4.12\n","Epoch 0: [900/2251], Loss: 4.10\n","Epoch 0: [950/2251], Loss: 4.08\n","Epoch 0: [1000/2251], Loss: 4.06\n","Epoch 0: [1050/2251], Loss: 4.04\n","Epoch 0: [1100/2251], Loss: 4.01\n","Epoch 0: [1150/2251], Loss: 4.00\n","Epoch 0: [1200/2251], Loss: 3.98\n","Epoch 0: [1250/2251], Loss: 3.97\n","Epoch 0: [1300/2251], Loss: 3.97\n","Epoch 0: [1350/2251], Loss: 3.98\n","Epoch 0: [1400/2251], Loss: 3.97\n","Epoch 0: [1450/2251], Loss: 3.96\n","Epoch 0: [1500/2251], Loss: 3.94\n","Epoch 0: [1550/2251], Loss: 3.93\n","Epoch 0: [1600/2251], Loss: 3.92\n","Epoch 0: [1650/2251], Loss: 3.91\n","Epoch 0: [1700/2251], Loss: 3.90\n","Epoch 0: [1750/2251], Loss: 3.89\n","Epoch 0: [1800/2251], Loss: 3.88\n","Epoch 0: [1850/2251], Loss: 3.88\n","Epoch 0: [1900/2251], Loss: 3.87\n","Epoch 0: [1950/2251], Loss: 3.86\n","Epoch 0: [2000/2251], Loss: 3.86\n","Epoch 0: [2050/2251], Loss: 3.85\n","Epoch 0: [2100/2251], Loss: 3.85\n","Epoch 0: [2150/2251], Loss: 3.84\n","Epoch 0: [2200/2251], Loss: 3.83\n","Epoch 0: [2250/2251], Loss: 3.82\n","Epoch 1: [0/2251], Loss: 3.84\n","Epoch 1: [50/2251], Loss: 3.64\n","Epoch 1: [100/2251], Loss: 3.68\n","Epoch 1: [150/2251], Loss: 3.62\n","Epoch 1: [200/2251], Loss: 3.51\n","Epoch 1: [250/2251], Loss: 3.51\n","Epoch 1: [300/2251], Loss: 3.50\n","Epoch 1: [350/2251], Loss: 3.48\n","Epoch 1: [400/2251], Loss: 3.48\n","Epoch 1: [450/2251], Loss: 3.48\n","Epoch 1: [500/2251], Loss: 3.50\n","Epoch 1: [550/2251], Loss: 3.51\n","Epoch 1: [600/2251], Loss: 3.51\n","Epoch 1: [650/2251], Loss: 3.53\n","Epoch 1: [700/2251], Loss: 3.54\n","Epoch 1: [750/2251], Loss: 3.54\n","Epoch 1: [800/2251], Loss: 3.54\n","Epoch 1: [850/2251], Loss: 3.54\n","Epoch 1: [900/2251], Loss: 3.54\n","Epoch 1: [950/2251], Loss: 3.54\n","Epoch 1: [1000/2251], Loss: 3.54\n","Epoch 1: [1050/2251], Loss: 3.53\n","Epoch 1: [1100/2251], Loss: 3.53\n","Epoch 1: [1150/2251], Loss: 3.53\n","Epoch 1: [1200/2251], Loss: 3.53\n","Epoch 1: [1250/2251], Loss: 3.53\n","Epoch 1: [1300/2251], Loss: 3.53\n","Epoch 1: [1350/2251], Loss: 3.53\n","Epoch 1: [1400/2251], Loss: 3.53\n","Epoch 1: [1450/2251], Loss: 3.53\n","Epoch 1: [1500/2251], Loss: 3.53\n","Epoch 1: [1550/2251], Loss: 3.53\n","Epoch 1: [1600/2251], Loss: 3.53\n","Epoch 1: [1650/2251], Loss: 3.53\n","Epoch 1: [1700/2251], Loss: 3.53\n","Epoch 1: [1750/2251], Loss: 3.53\n","Epoch 1: [1800/2251], Loss: 3.53\n","Epoch 1: [1850/2251], Loss: 3.53\n","Epoch 1: [1900/2251], Loss: 3.52\n","Epoch 1: [1950/2251], Loss: 3.52\n","Epoch 1: [2000/2251], Loss: 3.52\n","Epoch 1: [2050/2251], Loss: 3.52\n","Epoch 1: [2100/2251], Loss: 3.53\n","Epoch 1: [2150/2251], Loss: 3.53\n","Epoch 1: [2200/2251], Loss: 3.53\n","Epoch 1: [2250/2251], Loss: 3.52\n"]}],"source":["from torch.utils.data import DataLoader\n","\n","# 数据加载器\n","# train_loader = DataLoader(dataset['train'], batch_size=CFG.batch_size, shuffle=True)\n","train_loader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=True)\n","# 训练循环\n","num_epochs = 10\n","model.train()\n","\n","epoch_steps = len(train_loader)\n","print(f'trian_loader length: {epoch_steps}')\n","\n","best_loss = 1_000\n","for epoch in range(num_epochs):\n","    losses = []\n","    for i, batch in enumerate(train_loader):\n","        inputs = bert_tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=512)\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","        \n","        decoder_inputs = gpt2_tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=512)\n","        decoder_input_ids = decoder_inputs['input_ids']\n","        decoder_attention_mask = decoder_inputs['attention_mask']\n","\n","        input_ids, attention_mask, decoder_input_ids, decoder_attention_mask = input_ids.to(device), attention_mask.to(device), decoder_input_ids.to(device), decoder_attention_mask.to(device) \n","\n","        optimizer.zero_grad()\n","        recon_logits, kl_div = model(input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)\n","        loss = vae_loss_function(recon_logits, decoder_input_ids, kl_div)\n","\n","        loss.backward()\n","        optimizer.step()\n","        losses.append(loss.item())\n","        if i!=0 and i%50 == 0:\n","            print(f\"Epoch {epoch}: [{i}/{epoch_steps}], Loss: {np.mean(losses):.2f}\")\n","            if best_loss > np.mean(losses):\n","                model_path_tr = f'model_best_tr.pth'\n","                torch.save(model.state_dict(), model_path_tr)\n","                best_loss = np.mean(losses)\n","        \n","    model_path = f'model_epoch{epoch}.pth'\n","    torch.save(model.state_dict(), model_path)"]},{"cell_type":"markdown","metadata":{},"source":["# 5.推理"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-30T08:26:44.382649Z","iopub.status.idle":"2024-05-30T08:26:44.383087Z","shell.execute_reply":"2024-05-30T08:26:44.382887Z","shell.execute_reply.started":"2024-05-30T08:26:44.382868Z"},"trusted":true},"outputs":[],"source":["# underlying_model = model.module\n","\n","# underlying_model.eval()\n","\n","# with torch.no_grad():\n","#     sample_text = \"This is a test sentence.\"\n","#     inputs = bert_tokenizer(sample_text, return_tensors='pt')\n","#     input_ids = inputs['input_ids']\n","#     attention_mask = inputs['attention_mask']\n","\n","#     mu, logvar = underlying_model.encode(input_ids, attention_mask)\n","#     z = underlying_model.reparameterize(mu, logvar)\n","\n","#     # 使用起始token生成新文本\n","#     decoder_input_ids = gpt2_tokenizer(\"<|endoftext|>\", return_tensors='pt').input_ids\n","\n","#     # 将latent vector重复多次以匹配decoder_input_ids的序列长度\n","#     latent_hidden = z.unsqueeze(1).repeat(1, decoder_input_ids.size(1), 1)\n","\n","#     outputs = underlying_model.gpt2_decoder(inputs_embeds=latent_hidden, labels=decoder_input_ids)\n","#     predicted_ids = torch.argmax(outputs.logits, dim=-1)\n","\n","#     generated_text = gpt2_tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n","#     print(f\"Generated Text: {generated_text}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4969762,"sourceId":8361984,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
