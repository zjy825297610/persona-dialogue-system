{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Load packages"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-16T03:27:31.157759Z","iopub.status.busy":"2024-05-16T03:27:31.157396Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import BertTokenizer, BertModel\n","\n","import matplotlib.pyplot as plt\n","\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["# Wandb login & Config & Loading Data"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["#import wandb\n","#from kaggle_secrets import UserSecretsClient\n","#user_secrets = UserSecretsClient()\n","#secret_value_0 = user_secrets.get_secret(\"wandb_key\")\n","\n","#wandb.login(key = secret_value_0)"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["class config:\n","    seed = 42\n","    \n","    ## train parameters\n","    BATCH_SIZE = 64\n","    EPOCHES = 200\n","    num_warmup_rate=0.05\n","    n_clusters = 140\n","    model_name = 'BERT-MLP'"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["df_b = pd.read_csv('/home/zhaojinyue/workspace/intent-cluster/archive/data/banking/train.tsv', sep='\\t')\n","df_c = pd.read_csv('/home/zhaojinyue/workspace/intent-cluster/archive/data/clinc/train.tsv', sep='\\t')\n","df_b_te = pd.read_csv('/home/zhaojinyue/workspace/intent-cluster/archive/data/clinc/test.tsv', sep='\\t')\n","df_b_dev = pd.read_csv('/home/zhaojinyue/workspace/intent-cluster/archive/data/banking/dev.tsv', sep='\\t')"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Could you help my figure out the exchange fee?</td>\n","      <td>exchange_charge</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I made a cash deposit to my account but i don'...</td>\n","      <td>balance_not_updated_after_cheque_or_cash_deposit</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Hello - I'm on the app and trying to purchase ...</td>\n","      <td>beneficiary_not_allowed</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Why is it saying I have a pending payment?</td>\n","      <td>pending_card_payment</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Is there an extra charge to exchange different...</td>\n","      <td>exchange_charge</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  \\\n","0     Could you help my figure out the exchange fee?   \n","1  I made a cash deposit to my account but i don'...   \n","2  Hello - I'm on the app and trying to purchase ...   \n","3         Why is it saying I have a pending payment?   \n","4  Is there an extra charge to exchange different...   \n","\n","                                              label  \n","0                                   exchange_charge  \n","1  balance_not_updated_after_cheque_or_cash_deposit  \n","2                           beneficiary_not_allowed  \n","3                              pending_card_payment  \n","4                                   exchange_charge  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_b.head()"]},{"cell_type":"markdown","metadata":{},"source":["# class mapping dict"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["df = df_b\n","df_te = df_b_te\n","num_classes = df['label'].nunique()*2"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["label_mapping = {v:i for i,v in enumerate(df['label'].unique())}\n","df['label_num'] = df['label'].map(label_mapping)\n","df_te['label_num'] = df_te['label'].map(label_mapping)"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('/home/zhaojinyue/workspace/intent-cluster/bert')\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, dataframe, num_classes=20):\n","        self.data = dataframe\n","        self.num_classes = num_classes\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        text = self.data.loc[index, 'text']\n","        encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True, max_length=128, padding='max_length')\n","        label = self.data.loc[index, 'label_num']\n","        one_hot_label = F.one_hot(torch.tensor(label), num_classes=self.num_classes)\n","        inputs_ids = encoded_input['input_ids'].squeeze(0)\n","        attention_mask = encoded_input['attention_mask'].squeeze(0)\n","        return inputs_ids, attention_mask, one_hot_label"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["dataset = CustomDataset(df, num_classes)\n","val_dataset = CustomDataset(df_te, num_classes)"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["batch_size = config.BATCH_SIZE  # 批量大小\n","shuffle = True  # 打乱数据\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size*2)"]},{"cell_type":"markdown","metadata":{},"source":["# Build Model"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /home/zhaojinyue/workspace/intent-cluster/bert were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"text/plain":["BERTsemi(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (fc1): Linear(in_features=768, out_features=256, bias=True)\n","  (fc3): Linear(in_features=256, out_features=154, bias=True)\n","  (relu): ReLU()\n","  (softmax): Softmax(dim=1)\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["class BERTsemi(nn.Module):\n","    def __init__(self, num_classes):\n","        super(BERTsemi, self).__init__()\n","        self.bert = BertModel.from_pretrained('/home/zhaojinyue/workspace/intent-cluster/bert')\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc1 = nn.Linear(768, 256)\n","        self.fc3 = nn.Linear(256, num_classes)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","    \n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","#         pooled_output = outputs.last_hidden_states\n","        pooled_output = outputs.pooler_output\n","        pooled_output = self.dropout(pooled_output)\n","        hidden = self.relu(self.fc1(pooled_output))\n","        logits = self.softmax(self.fc3(hidden))\n","        return pooled_output,logits\n","\n","model = BERTsemi(num_classes=num_classes)\n","model_dict = model.state_dict()\n","pretrained_dict = torch.load('/home/zhaojinyue/workspace/intent-cluster/model_best_tr.pth',map_location=torch.device('cpu'))\n","pretrained_dict = {key: value for key, value in pretrained_dict.items() if key in model_dict }\n","model_dict.update(pretrained_dict)\n","model.load_state_dict(model_dict)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["# wandb.init() & loss & optim & scheduler"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectTimeout), entering retry loop.\n","wandb: Network error (ConnectTimeout), entering retry loop.\n","\u001b[34m\u001b[1mwandb\u001b[0m: - Waiting for wandb.init()...\r"]},{"ename":"CommError","evalue":"Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m((name, \u001b[38;5;28mgetattr\u001b[39m(f, name)) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(f) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNLP-intent-BERT-semi-supuervised\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass2dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43manonymous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmust\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1178\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, settings)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=1173'>1174</a>\u001b[0m     logger\u001b[39m.\u001b[39mexception(\u001b[39m\"\u001b[39m\u001b[39merror in wandb.init()\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=1175'>1176</a>\u001b[0m \u001b[39m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=1176'>1177</a>\u001b[0m \u001b[39m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=1177'>1178</a>\u001b[0m wandb\u001b[39m.\u001b[39;49m_sentry\u001b[39m.\u001b[39;49mreraise(e)\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=1178'>1179</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m()\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/wandb/analytics/sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/analytics/sentry.py?line=151'>152</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexception(exc)\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/analytics/sentry.py?line=152'>153</a>\u001b[0m \u001b[39m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/analytics/sentry.py?line=153'>154</a>\u001b[0m \u001b[39m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/analytics/sentry.py?line=154'>155</a>\u001b[0m \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m])\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1164\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, settings)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=1161'>1162</a>\u001b[0m     wi \u001b[39m=\u001b[39m _WandbInit()\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=1162'>1163</a>\u001b[0m     wi\u001b[39m.\u001b[39msetup(kwargs)\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=1163'>1164</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=1165'>1166</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=1166'>1167</a>\u001b[0m     \u001b[39mif\u001b[39;00m logger \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:776\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=773'>774</a>\u001b[0m         backend\u001b[39m.\u001b[39mcleanup()\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=774'>775</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mteardown()\n\u001b[0;32m--> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=775'>776</a>\u001b[0m     \u001b[39mraise\u001b[39;00m error\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=777'>778</a>\u001b[0m \u001b[39massert\u001b[39;00m run_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# for mypy\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/wandb/sdk/wandb_init.py?line=779'>780</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m run_result\u001b[39m.\u001b[39mHasField(\u001b[39m\"\u001b[39m\u001b[39mrun\u001b[39m\u001b[39m\"\u001b[39m):\n","\u001b[0;31mCommError\u001b[0m: Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-"]}],"source":["def class2dict(f):\n","    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n","import wandb\n","wandb.init(project='NLP-intent-BERT-semi-supuervised', \n","    name=config.model_name,\n","    config=class2dict(config),\n","    group=config.model_name,\n","    job_type=\"train\",\n","    anonymous=\"must\")"]},{"cell_type":"markdown","metadata":{},"source":["## Define Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class SupConLoss(nn.Module):\n","    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n","    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n","    def __init__(self, contrast_mode='all'):\n","        super(SupConLoss, self).__init__()\n","        self.contrast_mode = contrast_mode\n","\n","    def forward(self, features, labels=None, mask=None, temperature = 0.07, device = None):\n","        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n","        it degenerates to SimCLR unsupervised loss:\n","        https://arxiv.org/pdf/2002.05709.pdf\n","        Args:\n","            features: hidden vector of shape [bsz, n_views, ...].\n","            labels: ground truth of shape [bsz].\n","            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n","                has the same class as sample i. Can be asymmetric.\n","        Returns:\n","            A loss scalar.\n","        \"\"\"\n","\n","        if len(features.shape) < 3:\n","            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n","                             'at least 3 dimensions are required')\n","        if len(features.shape) > 3:\n","            features = features.view(features.shape[0], features.shape[1], -1)\n","\n","        batch_size = features.shape[0]\n","        if labels is not None and mask is not None:\n","            raise ValueError('Cannot define both `labels` and `mask`')\n","        elif labels is None and mask is None:\n","            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n","        elif labels is not None:\n","            labels = labels.contiguous().view(-1, 1)\n","            if labels.shape[0] != batch_size:\n","                raise ValueError('Num of labels does not match num of features')\n","            mask = torch.eq(labels, labels.T).float().to(device)\n","        else:\n","            mask = mask.float().to(device)\n","\n","        contrast_count = features.shape[1]\n","        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n","        if self.contrast_mode == 'one':\n","            anchor_feature = features[:, 0]\n","            anchor_count = 1\n","        elif self.contrast_mode == 'all':\n","            anchor_feature = contrast_feature\n","            anchor_count = contrast_count\n","        else:\n","            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n","\n","        # compute logits\n","        anchor_dot_contrast = torch.div(\n","            torch.matmul(anchor_feature, contrast_feature.T),\n","            temperature)\n","        # for numerical stability\n","        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n","        logits = anchor_dot_contrast - logits_max.detach()\n","\n","        # tile mask\n","        mask = mask.repeat(anchor_count, contrast_count)\n","        # mask-out self-contrast cases\n","        logits_mask = torch.scatter(\n","            torch.ones_like(mask),\n","            1,\n","            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n","            0\n","        )\n","        mask = mask * logits_mask\n","\n","        # compute log_prob\n","        exp_logits = torch.exp(logits) * logits_mask\n","        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n","\n","        # compute mean of log-likelihood over positive\n","        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n","\n","        # loss\n","        loss = - mean_log_prob_pos\n","        loss = loss.view(anchor_count, batch_size).mean()\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-28 09:52:21.595387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["each_epoch_steps = len(dataloader)\n","epoches = config.EPOCHES\n","\n","criterion = nn.CrossEntropyLoss()\n","contrast_criterion = SupConLoss()\n","\n","optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-6)\n","l_optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-6)\n","\n","from transformers import get_cosine_schedule_with_warmup\n","num_train_steps = config.EPOCHES*each_epoch_steps\n","num_warmup_steps = int(num_train_steps*config.num_warmup_rate)\n","## scheduler\n","scheduler = get_cosine_schedule_with_warmup(\n","    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps, num_cycles=0.5)\n","\n","l_scheduler = get_cosine_schedule_with_warmup(\n","    l_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps, num_cycles=0.5)"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","feat_dim = 768"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from tqdm import tqdm\n","from tqdm.notebook import tqdm\n","\n","def get_outputs(mode, model):    \n","    if mode == 'test':\n","        dataloader_ = val_dataloader\n","    elif mode == 'train':\n","        dataloader_ = dataloader\n","    model.eval()\n","\n","    total_labels = torch.empty(0,dtype=torch.float).to(device)#创建空list\n","    total_preds = torch.empty(0,dtype=torch.long).to(device)\n","        \n","    total_features = torch.empty((0, feat_dim)).to(device)\n","    total_logits = torch.empty((0, num_classes)).to(device)\n","    \n","    for input_ids, attention_mask, labels in tqdm(dataloader_, desc=\"Iteration\"):\n","        input_ids, attention_mask, labels = input_ids.to(device, dtype=torch.long), attention_mask.to(device, dtype=torch.long), labels.to(device, dtype=torch.float)\n","\n","        with torch.set_grad_enabled(False):\n","            feats,logits = model(input_ids, attention_mask)\n","                \n","            total_labels = torch.cat((total_labels,labels.argmax(axis=1)))\n","            total_features = torch.cat((total_features, feats))\n","            total_logits = torch.cat((total_logits, logits))\n","        \n","    feats = total_features.cpu().numpy()\n","    y_true = total_labels.cpu().numpy()\n","        \n","    total_probs = F.softmax(total_logits.detach(), dim=1)\n","    total_maxprobs, total_preds = total_probs.max(dim = 1)\n","    y_pred = total_preds.cpu().numpy()\n","        \n","    y_logits = total_logits.cpu().numpy()\n","        \n","    outputs = {\n","        'y_true': y_true,\n","        'y_pred': y_pred,\n","        'logits': y_logits,\n","        'feats': feats\n","    }\n","    return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## dui\n","def clustering(model):\n","    outputs = get_outputs(mode = 'train', model = model)\n","    feats = outputs['feats']\n","    y_true = outputs['y_true']\n","        \n","    labeled_pos = list(np.where(y_true != -1)[0])\n","    labeled_feats = feats[labeled_pos]\n","    labeled_labels = y_true[labeled_pos]        \n","    labeled_centers = []\n","    for idx, label in enumerate(np.unique(labeled_labels)):\n","        label_feats = labeled_feats[labeled_labels == label]\n","        labeled_centers.append(np.mean(label_feats, axis = 0))\n","        \n","    km = KMeans(n_clusters = num_classes, random_state=config.seed, init = 'k-means++').fit(feats) \n","    km_centroids, assign_labels = km.cluster_centers_, km.labels_\n","         \n","    centroids = torch.tensor(km_centroids).to(device)\n","    pseudo_labels = assign_labels.astype(np.int64)\n","        \n","    return outputs, km_centroids, y_true, assign_labels, pseudo_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.68 GiB of which 91.75 MiB is free. Process 11160 has 5.08 GiB memory in use. Including non-PyTorch memory, this process has 18.49 GiB memory in use. Of the allocated memory 17.96 GiB is allocated by PyTorch, and 228.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong), attention_mask\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong), labels\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     10\u001b[0m feats_a,logits_a \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[0;32m---> 11\u001b[0m feats_b,logits_b \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m norm_feats_a \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(feats_a)\n\u001b[1;32m     14\u001b[0m norm_feats_b \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(feats_b)\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mBERTsemi.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m---> 12\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#         pooled_output = outputs.last_hidden_states\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1010'>1011</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1012'>1013</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1013'>1014</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1014'>1015</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1017'>1018</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1018'>1019</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1019'>1020</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1020'>1021</a>\u001b[0m     embedding_output,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1021'>1022</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1022'>1023</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1023'>1024</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1024'>1025</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1025'>1026</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1026'>1027</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1027'>1028</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1028'>1029</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1029'>1030</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1030'>1031</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1031'>1032</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=1032'>1033</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=600'>601</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=601'>602</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=602'>603</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=606'>607</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=607'>608</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=608'>609</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=609'>610</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=610'>611</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=611'>612</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=612'>613</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=613'>614</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=614'>615</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=615'>616</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=616'>617</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=617'>618</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=619'>620</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=620'>621</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=533'>534</a>\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=534'>535</a>\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=536'>537</a>\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=537'>538</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=538'>539</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=539'>540</a>\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=541'>542</a>\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/pytorch_utils.py?line=232'>233</a>\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/pytorch_utils.py?line=233'>234</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/pytorch_utils.py?line=235'>236</a>\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:549\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=547'>548</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=548'>549</a>\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=549'>550</a>\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=550'>551</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:450\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=447'>448</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=448'>449</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=449'>450</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_act_fn(hidden_states)\n\u001b[1;32m    <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py?line=450'>451</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/conda/yes/lib/python3.11/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/activations.py?line=76'>77</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///home/zhaojinyue/conda/yes/lib/python3.11/site-packages/transformers/activations.py?line=77'>78</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.68 GiB of which 91.75 MiB is free. Process 11160 has 5.08 GiB memory in use. Including non-PyTorch memory, this process has 18.49 GiB memory in use. Of the allocated memory 17.96 GiB is allocated by PyTorch, and 228.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["last_preds = None\n","\n","for epoch in range(epoches):\n","    ## train\n","    model.train()\n","    st = time.time()\n","    for input_ids, attention_mask, labels in dataloader:\n","        input_ids, attention_mask, labels = input_ids.to(device, dtype=torch.long), attention_mask.to(device, dtype=torch.long), labels.to(device, dtype=torch.float)\n","\n","        feats_a,logits_a = model(input_ids, attention_mask)\n","        feats_b,logits_b = model(input_ids, attention_mask)\n","\n","        norm_feats_a = F.normalize(feats_a)\n","        norm_feats_b = F.normalize(feats_b)\n","        \n","        constrastive_feats = torch.cat((norm_feats_a.unsqueeze(1), norm_feats_b.unsqueeze(1)), dim = 1)\n","        \n","        ## 计算对比学习Loss，使用的simCLR 的loss https://arxiv.org/pdf/2002.05709.pdf\n","        loss_contrast = contrast_criterion(constrastive_feats, labels = labels.argmax(axis=1), temperature = 0.07, device = device)\n","        \n","        loss = loss_contrast\n","        \n","        loss.backward()\n","        l_optimizer.step()\n","        l_optimizer.zero_grad()        \n","        l_scheduler.step()\n","        \n","    ## 更新质心和伪标签\n","    outputs, km_centroids, y_true, assign_labels, pseudo_labels = clustering(model)\n","    \n","    current_preds = pseudo_labels\n","    \n","    ## 计算当前 两次伪标签的距离小于某个值时可以用于提前停止，目前暂未使用\n","#     delta_label = np.sum(current_preds != last_preds).astype(np.float32)/ current_preds.shape[0]\n","#     last_preds = np.copy(current_preds)\n","    \n","    ## 质心引导，对比学习训练（此处输入label为伪标签）\n","    losses2 = []\n","    model.train()\n","    for i, (input_ids, attention_mask, labels) in enumerate(dataloader):\n","        labels_ = torch.tensor(pseudo_labels[batch_size*i:batch_size*(i+1)])\n","        labels_ = F.one_hot(labels_, num_classes=num_classes)\n","        input_ids, attention_mask, labels_ = input_ids.to(device, dtype=torch.long), attention_mask.to(device, dtype=torch.long), labels_.to(device, dtype=torch.float)\n","        # random\n","        feats_a,logits_a = model(input_ids, attention_mask)\n","        feats_b,logits_b = model(input_ids, attention_mask)\n","    \n","        norm_feats_a = F.normalize(feats_a)\n","        norm_feats_b = F.normalize(feats_b)\n","        \n","        ## 计算对比学习\n","        constrastive_feats = torch.cat((norm_feats_a.unsqueeze(1), norm_feats_b.unsqueeze(1)), dim = 1)\n","        loss_contrast = contrast_criterion(constrastive_feats, labels = labels_.argmax(axis=1), temperature = 0.07, device = device)\n","        \n","        ## 伪标签与预测差异loss\n","        loss_ce = 0.5 * (criterion(logits_a, labels_) + criterion(logits_b, labels_)) \n","                    \n","        loss = loss_contrast + loss_ce\n","\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        losses2.append(loss.item())\n","        scheduler.step()\n","    ed = time.time()\n","    print(f'[Epoch {epoch+1}/{epoches}] Train Loss: {np.mean(losses2):.2f}, time: {ed-st:.0f}s')\n","    # wandb\n","    wandb.log({\n","        f\"Epoch\": epoch+1,\n","        f\"avg_train_loss\": np.mean(losses2),\n","    })"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4969762,"sourceId":8361984,"sourceType":"datasetVersion"},{"datasetId":4996986,"sourceId":8398970,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
