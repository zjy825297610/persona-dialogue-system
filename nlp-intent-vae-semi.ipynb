{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1. 参数配置"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-30T09:20:58.323034Z","iopub.status.busy":"2024-05-30T09:20:58.322672Z","iopub.status.idle":"2024-05-30T09:21:06.917025Z","shell.execute_reply":"2024-05-30T09:21:06.916054Z","shell.execute_reply.started":"2024-05-30T09:20:58.323006Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel\n","from torch.distributions import MultivariateNormal, Categorical\n","\n","import matplotlib.pyplot as plt\n","\n","import time"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:06.919259Z","iopub.status.busy":"2024-05-30T09:21:06.918757Z","iopub.status.idle":"2024-05-30T09:21:10.026957Z","shell.execute_reply":"2024-05-30T09:21:10.025909Z","shell.execute_reply.started":"2024-05-30T09:21:06.919220Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["## wandb login\n","import wandb\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","# secret_value_0 = user_secrets.get_secret(\"wandb_key\")\n","\n","wandb.login(key = secret_value_0)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:10.033008Z","iopub.status.busy":"2024-05-30T09:21:10.032654Z","iopub.status.idle":"2024-05-30T09:21:10.038152Z","shell.execute_reply":"2024-05-30T09:21:10.037156Z","shell.execute_reply.started":"2024-05-30T09:21:10.032969Z"},"trusted":true},"outputs":[],"source":["## Config\n","class config:\n","    seed = 42\n","    \n","    ## train parameters\n","    BATCH_SIZE = 64\n","    EPOCHES = 200\n","    num_warmup_rate=0.05\n","    n_clusters = 140\n","    model_name = 'VAE+BN'"]},{"cell_type":"markdown","metadata":{},"source":["# 2. 数据准备"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:10.039881Z","iopub.status.busy":"2024-05-30T09:21:10.039580Z","iopub.status.idle":"2024-05-30T09:21:10.334910Z","shell.execute_reply":"2024-05-30T09:21:10.333508Z","shell.execute_reply.started":"2024-05-30T09:21:10.039857Z"},"trusted":true},"outputs":[],"source":["## Load data\n","df_b = pd.read_csv('/kaggle/input/intent-dataset/data/banking/train.tsv', sep='\\t')\n","df_c = pd.read_csv('/kaggle/input/intent-dataset/data/clinc/train.tsv', sep='\\t')\n","df_b_te = pd.read_csv('/kaggle/input/intent-dataset/data/banking/test.tsv', sep='\\t')\n","df_b_dev = pd.read_csv('/kaggle/input/intent-dataset/data/banking/dev.tsv', sep='\\t')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:10.338012Z","iopub.status.busy":"2024-05-30T09:21:10.337137Z","iopub.status.idle":"2024-05-30T09:21:10.349199Z","shell.execute_reply":"2024-05-30T09:21:10.348111Z","shell.execute_reply.started":"2024-05-30T09:21:10.337967Z"},"trusted":true},"outputs":[],"source":["df = df_b\n","df_te = df_b_te\n","num_classes = df['label'].nunique()#*2"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:10.351048Z","iopub.status.busy":"2024-05-30T09:21:10.350658Z","iopub.status.idle":"2024-05-30T09:21:10.366332Z","shell.execute_reply":"2024-05-30T09:21:10.365132Z","shell.execute_reply.started":"2024-05-30T09:21:10.351009Z"},"trusted":true},"outputs":[],"source":["label_mapping = {v:i for i,v in enumerate(df['label'].unique())}\n","df['label_num'] = df['label'].map(label_mapping)\n","df_te['label_num'] = df_te['label'].map(label_mapping)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:10.367925Z","iopub.status.busy":"2024-05-30T09:21:10.367602Z","iopub.status.idle":"2024-05-30T09:21:11.119726Z","shell.execute_reply":"2024-05-30T09:21:11.118872Z","shell.execute_reply.started":"2024-05-30T09:21:10.367898Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"095bed86017b456982dd5953288b509e","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b7c7ed2bdbe4e80aec0c4b9d8877329","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd515c69c5fc4b528e8a67d67866157a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"368c513f34b440fca89d04dba85d4102","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, dataframe, num_classes=20):\n","        self.data = dataframe\n","        self.num_classes = num_classes\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        text = self.data.loc[index, 'text']\n","        encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True, max_length=128, padding='max_length')\n","        label = self.data.loc[index, 'label_num']\n","        one_hot_label = F.one_hot(torch.tensor(label), num_classes=self.num_classes)\n","        inputs_ids = encoded_input['input_ids'].squeeze(0)\n","        attention_mask = encoded_input['attention_mask'].squeeze(0)\n","        return inputs_ids, attention_mask, one_hot_label"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:11.121176Z","iopub.status.busy":"2024-05-30T09:21:11.120889Z","iopub.status.idle":"2024-05-30T09:21:11.125971Z","shell.execute_reply":"2024-05-30T09:21:11.124831Z","shell.execute_reply.started":"2024-05-30T09:21:11.121152Z"},"trusted":true},"outputs":[],"source":["dataset = CustomDataset(df, num_classes)\n","val_dataset = CustomDataset(df_te, num_classes)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:11.132047Z","iopub.status.busy":"2024-05-30T09:21:11.131227Z","iopub.status.idle":"2024-05-30T09:21:11.137863Z","shell.execute_reply":"2024-05-30T09:21:11.136889Z","shell.execute_reply.started":"2024-05-30T09:21:11.132005Z"},"trusted":true},"outputs":[],"source":["batch_size = config.BATCH_SIZE  # 批量大小\n","shuffle = True  # 打乱数据\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size*2)"]},{"cell_type":"markdown","metadata":{},"source":["# 3.模型定义"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:11.139742Z","iopub.status.busy":"2024-05-30T09:21:11.139359Z","iopub.status.idle":"2024-05-30T09:21:21.615559Z","shell.execute_reply":"2024-05-30T09:21:21.614475Z","shell.execute_reply.started":"2024-05-30T09:21:11.139708Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"535d51d60d5e4f7caa5c822dc23331fe","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["VAE_DEC(\n","  (bert_encoder): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (fc_mu): Linear(in_features=768, out_features=768, bias=True)\n","  (fc_logvar): Linear(in_features=768, out_features=768, bias=True)\n","  (bn_mu): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (fc1): Linear(in_features=768, out_features=256, bias=True)\n","  (fc2): Linear(in_features=256, out_features=77, bias=True)\n","  (relu): ReLU()\n","  (softmax): Softmax(dim=1)\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# 加载BERT和GPT-2\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","\n","class VAE_DEC(nn.Module):\n","    def __init__(self, bert_model, num_classes):\n","        super(VAE_DEC, self).__init__()\n","        self.bert_encoder = bert_model\n","        self.latent_dim = bert_model.config.hidden_size\n","        \n","        self.fc_mu = nn.Linear(self.latent_dim, self.latent_dim)\n","        self.fc_logvar = nn.Linear(self.latent_dim, self.latent_dim)\n","        self.bn_mu = nn.BatchNorm1d(self.latent_dim)  # 添加批量归一化层\n","        \n","        self.dropout = nn.Dropout(0.1)\n","        self.fc1 = nn.Linear(self.latent_dim, 256)\n","        self.fc2 = nn.Linear(256, num_classes)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)        \n","        \n","    def encode(self, input_ids, attention_mask=None):\n","        outputs = self.bert_encoder(input_ids, attention_mask=attention_mask)\n","        hidden_state = outputs.last_hidden_state[:, 0, :]  # 使用[CLS] token表示\n","#         mu = self.fc_mu(hidden_state)\n","        mu = self.bn_mu(self.fc_mu(hidden_state))\n","#         logvar = self.fc_logvar(hidden_state)\n","        return mu\n","    \n","#     def reparameterize(self, mu, logvar):\n","#         std = torch.exp(0.5 * logvar)\n","#         eps = torch.randn_like(std)\n","#         return mu + eps * std\n","        \n","    def forward(self, input_ids, attention_mask):\n","        mu = self.encode(input_ids, attention_mask)\n","#         mu = self.dropout(z)\n","        hidden = self.relu(self.fc1(mu))\n","        logits = self.softmax(self.fc2(hidden))\n","        return mu, logits\n","\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","\n","model = VAE_DEC(bert_model, num_classes = num_classes )\n","\n","model_dict = model.state_dict()\n","\n","pretrained_dict = torch.load('/kaggle/input/nlp-intent-vae-dec/model_epoch2.pth',map_location=torch.device('cpu'))\n","# pretrained_dict = torch.load('/kaggle/input/nlp-intent-vae-dec/model_epoch1.pth',map_location=torch.device('cpu'))\n","pretrained_dict = {key: value for key, value in pretrained_dict.items() if key in model_dict }\n","model_dict.update(pretrained_dict)\n","model.load_state_dict(model_dict)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["# 3.损失函数和优化器"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:21.617293Z","iopub.status.busy":"2024-05-30T09:21:21.616931Z","iopub.status.idle":"2024-05-30T09:21:38.879297Z","shell.execute_reply":"2024-05-30T09:21:38.878259Z","shell.execute_reply.started":"2024-05-30T09:21:21.617263Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmengvision\u001b[0m (\u001b[33mnumberist\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240530_092121-uiiaz45b</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/numberist/NLP-intent-VAE-supuervised/runs/uiiaz45b' target=\"_blank\">VAE+BN</a></strong> to <a href='https://wandb.ai/numberist/NLP-intent-VAE-supuervised' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/numberist/NLP-intent-VAE-supuervised' target=\"_blank\">https://wandb.ai/numberist/NLP-intent-VAE-supuervised</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/numberist/NLP-intent-VAE-supuervised/runs/uiiaz45b' target=\"_blank\">https://wandb.ai/numberist/NLP-intent-VAE-supuervised/runs/uiiaz45b</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/numberist/NLP-intent-VAE-supuervised/runs/uiiaz45b?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x799d12eca2f0>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["## wandb.init()\n","def class2dict(f):\n","    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n","\n","wandb.init(project='NLP-intent-VAE-supuervised', \n","    name=config.model_name,\n","    config=class2dict(config),\n","    group=config.model_name,\n","    job_type=\"train\",\n","    anonymous=\"must\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:38.881533Z","iopub.status.busy":"2024-05-30T09:21:38.880807Z","iopub.status.idle":"2024-05-30T09:21:38.900553Z","shell.execute_reply":"2024-05-30T09:21:38.899269Z","shell.execute_reply.started":"2024-05-30T09:21:38.881503Z"},"trusted":true},"outputs":[],"source":["## SupConLoss Define\n","class SupConLoss(nn.Module):\n","    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n","    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n","    def __init__(self, contrast_mode='all'):\n","        super(SupConLoss, self).__init__()\n","        self.contrast_mode = contrast_mode\n","\n","    def forward(self, features, labels=None, mask=None, temperature = 0.07, device = None):\n","        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n","        it degenerates to SimCLR unsupervised loss:\n","        https://arxiv.org/pdf/2002.05709.pdf\n","        Args:\n","            features: hidden vector of shape [bsz, n_views, ...].\n","            labels: ground truth of shape [bsz].\n","            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n","                has the same class as sample i. Can be asymmetric.\n","        Returns:\n","            A loss scalar.\n","        \"\"\"\n","\n","        if len(features.shape) < 3:\n","            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n","                             'at least 3 dimensions are required')\n","        if len(features.shape) > 3:\n","            features = features.view(features.shape[0], features.shape[1], -1)\n","\n","        batch_size = features.shape[0]\n","        if labels is not None and mask is not None:\n","            raise ValueError('Cannot define both `labels` and `mask`')\n","        elif labels is None and mask is None:\n","            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n","        elif labels is not None:\n","            labels = labels.contiguous().view(-1, 1)\n","            if labels.shape[0] != batch_size:\n","                raise ValueError('Num of labels does not match num of features')\n","            mask = torch.eq(labels, labels.T).float().to(device)\n","        else:\n","            mask = mask.float().to(device)\n","\n","        contrast_count = features.shape[1]\n","        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n","        if self.contrast_mode == 'one':\n","            anchor_feature = features[:, 0]\n","            anchor_count = 1\n","        elif self.contrast_mode == 'all':\n","            anchor_feature = contrast_feature\n","            anchor_count = contrast_count\n","        else:\n","            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n","\n","        # compute logits\n","        anchor_dot_contrast = torch.div(\n","            torch.matmul(anchor_feature, contrast_feature.T),\n","            temperature)\n","        # for numerical stability\n","        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n","        logits = anchor_dot_contrast - logits_max.detach()\n","\n","        # tile mask\n","        mask = mask.repeat(anchor_count, contrast_count)\n","        # mask-out self-contrast cases\n","        logits_mask = torch.scatter(\n","            torch.ones_like(mask),\n","            1,\n","            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n","            0\n","        )\n","        mask = mask * logits_mask\n","\n","        # compute log_prob\n","        exp_logits = torch.exp(logits) * logits_mask\n","        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n","\n","        # compute mean of log-likelihood over positive\n","        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n","\n","        # loss\n","        loss = - mean_log_prob_pos\n","        loss = loss.view(anchor_count, batch_size).mean()\n","\n","        return loss"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:38.902029Z","iopub.status.busy":"2024-05-30T09:21:38.901757Z","iopub.status.idle":"2024-05-30T09:21:38.915888Z","shell.execute_reply":"2024-05-30T09:21:38.914545Z","shell.execute_reply.started":"2024-05-30T09:21:38.902006Z"},"trusted":true},"outputs":[],"source":["## Loss\n","each_epoch_steps = len(dataloader)\n","epoches = config.EPOCHES\n","\n","criterion = nn.CrossEntropyLoss()\n","contrast_criterion = SupConLoss()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:38.918510Z","iopub.status.busy":"2024-05-30T09:21:38.917405Z","iopub.status.idle":"2024-05-30T09:21:38.940998Z","shell.execute_reply":"2024-05-30T09:21:38.940258Z","shell.execute_reply.started":"2024-05-30T09:21:38.918475Z"},"trusted":true},"outputs":[],"source":["## Optimizer\n","optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-6)\n","l_optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-6)\n","\n","## Scheduler\n","from transformers import get_cosine_schedule_with_warmup\n","num_train_steps = config.EPOCHES*each_epoch_steps\n","num_warmup_steps = int(num_train_steps*config.num_warmup_rate)\n","scheduler = get_cosine_schedule_with_warmup(\n","    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps, num_cycles=0.5)\n","\n","l_scheduler = get_cosine_schedule_with_warmup(\n","    l_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps, num_cycles=0.5)"]},{"cell_type":"markdown","metadata":{},"source":["# 4.训练模型"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:38.942318Z","iopub.status.busy":"2024-05-30T09:21:38.942015Z","iopub.status.idle":"2024-05-30T09:21:39.943959Z","shell.execute_reply":"2024-05-30T09:21:39.942701Z","shell.execute_reply.started":"2024-05-30T09:21:38.942293Z"},"trusted":true},"outputs":[],"source":["from sklearn.cluster import KMeans\n","from tqdm.notebook import tqdm\n","\n","feat_dim = 768"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:39.946159Z","iopub.status.busy":"2024-05-30T09:21:39.945707Z","iopub.status.idle":"2024-05-30T09:21:39.964486Z","shell.execute_reply":"2024-05-30T09:21:39.963086Z","shell.execute_reply.started":"2024-05-30T09:21:39.946118Z"},"trusted":true},"outputs":[],"source":["def get_outputs(mode, model):    \n","    if mode == 'test':\n","        dataloader_ = val_dataloader\n","    elif mode == 'train':\n","        dataloader_ = dataloader\n","    model.eval()\n","\n","    total_labels = torch.empty(0,dtype=torch.float).to(device)#创建空list\n","    total_preds = torch.empty(0,dtype=torch.long).to(device)\n","        \n","    total_features = torch.empty((0, feat_dim)).to(device)\n","    total_logits = torch.empty((0, num_classes)).to(device)\n","    \n","    for input_ids, attention_mask, labels in tqdm(dataloader_, desc=\"Iteration\"):\n","        input_ids, attention_mask, labels = input_ids.to(device, dtype=torch.long), attention_mask.to(device, dtype=torch.long), labels.to(device, dtype=torch.float)\n","\n","        with torch.set_grad_enabled(False):\n","            feats,logits = model(input_ids, attention_mask)\n","                \n","            total_labels = torch.cat((total_labels,labels.argmax(axis=1)))\n","            total_features = torch.cat((total_features, feats))\n","            total_logits = torch.cat((total_logits, logits))\n","        \n","    feats = total_features.cpu().numpy()\n","    y_true = total_labels.cpu().numpy()\n","        \n","    total_probs = F.softmax(total_logits.detach(), dim=1)\n","    total_maxprobs, total_preds = total_probs.max(dim = 1)\n","    y_pred = total_preds.cpu().numpy()\n","        \n","    y_logits = total_logits.cpu().numpy()\n","        \n","    outputs = {\n","        'y_true': y_true,\n","        'y_pred': y_pred,\n","        'logits': y_logits,\n","        'feats': feats\n","    }\n","    return outputs"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:39.967236Z","iopub.status.busy":"2024-05-30T09:21:39.966145Z","iopub.status.idle":"2024-05-30T09:21:39.976149Z","shell.execute_reply":"2024-05-30T09:21:39.974676Z","shell.execute_reply.started":"2024-05-30T09:21:39.967183Z"},"trusted":true},"outputs":[],"source":["# outputs = get_outputs(mode = 'test', model = model)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:39.977936Z","iopub.status.busy":"2024-05-30T09:21:39.977575Z","iopub.status.idle":"2024-05-30T09:21:39.988232Z","shell.execute_reply":"2024-05-30T09:21:39.987255Z","shell.execute_reply.started":"2024-05-30T09:21:39.977911Z"},"trusted":true},"outputs":[],"source":["def clustering(model):\n","    outputs = get_outputs(mode = 'train', model = model)\n","    feats = outputs['feats']\n","    y_true = outputs['y_true']\n","        \n","    labeled_pos = list(np.where(y_true != -1)[0])\n","    labeled_feats = feats[labeled_pos]\n","    labeled_labels = y_true[labeled_pos]        \n","    labeled_centers = []\n","    for idx, label in enumerate(np.unique(labeled_labels)):\n","        label_feats = labeled_feats[labeled_labels == label]\n","        labeled_centers.append(np.mean(label_feats, axis = 0))\n","        \n","    km = KMeans(n_clusters=num_classes, random_state=config.seed, init='k-means++').fit(feats) \n","    km_centroids, assign_labels = km.cluster_centers_, km.labels_\n","         \n","    centroids = torch.tensor(km_centroids).to(device)\n","    pseudo_labels = assign_labels.astype(np.int64)\n","        \n","    return outputs, km_centroids, y_true, assign_labels, pseudo_labels"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:39.989966Z","iopub.status.busy":"2024-05-30T09:21:39.989673Z","iopub.status.idle":"2024-05-30T09:21:40.001313Z","shell.execute_reply":"2024-05-30T09:21:40.000103Z","shell.execute_reply.started":"2024-05-30T09:21:39.989943Z"},"trusted":true},"outputs":[],"source":["## evalute\n","from sklearn.metrics.cluster import normalized_mutual_info_score\n","from sklearn.metrics import adjusted_rand_score\n","from sklearn.metrics import accuracy_score\n","\n","# 定义真实标签\n","true_labels = [0, 0, 1, 1, 1]\n","\n","# 定义聚类结果\n","cluster_labels = [0, 0, 1, 1, 1]\n","\n","def evalute(true_labels, cluster_labels):\n","    # 计算归一化互信息\n","    nmi = normalized_mutual_info_score(true_labels, cluster_labels, average_method='arithmetic')\n","    # 计算调整兰德系数\n","    ari = adjusted_rand_score(true_labels, cluster_labels)\n","    # 计算聚类准确率\n","    acc = accuracy_score(true_labels, cluster_labels)\n","\n","#     print(f\"Normalized Mutual Information (NMI): {nmi:.3f}\")\n","#     print(f\"Adjusted Rand Index (ARI): {ari:.3f}\")\n","#     print(f\"Clustering Accuracy (ACC): {acc:.3f}\")\n","    return nmi, ari, acc\n","\n","# nmi, ari, acc = evalute(true_labels, cluster_labels)\n","\n","# https://arxiv.org/pdf/2304.07699\n","# ===========================\n","# | NMI    | ARI    | ACC   |\n","# | 87.41  | 69.54  | 78.36 |\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:40.003549Z","iopub.status.busy":"2024-05-30T09:21:40.002536Z","iopub.status.idle":"2024-05-30T09:21:40.014565Z","shell.execute_reply":"2024-05-30T09:21:40.012907Z","shell.execute_reply.started":"2024-05-30T09:21:40.003509Z"},"trusted":true},"outputs":[],"source":["# ## Traing\n","# last_preds = None\n","\n","# for epoch in range(epoches):\n","#     ## train\n","#     model.train()\n","#     st = time.time()\n","#     losses = []\n","#     for input_ids, attention_mask, labels in dataloader:\n","#         input_ids, attention_mask, labels = input_ids.to(device, dtype=torch.long), attention_mask.to(device, dtype=torch.long), labels.to(device, dtype=torch.float)\n","\n","#         feats_a,logits_a = model(input_ids, attention_mask)\n","#         feats_b,logits_b = model(input_ids, attention_mask)\n","\n","#         norm_feats_a = F.normalize(feats_a)\n","#         norm_feats_b = F.normalize(feats_b)\n","        \n","#         constrastive_feats = torch.cat((norm_feats_a.unsqueeze(1), norm_feats_b.unsqueeze(1)), dim = 1)\n","        \n","#         ## 计算对比学习Loss，使用的simCLR 的loss https://arxiv.org/pdf/2002.05709.pdf\n","#         loss_contrast = contrast_criterion(constrastive_feats, labels = labels.argmax(axis=1), temperature = 0.07, device = device)\n","        \n","        \n","#         loss = loss_contrast\n","        \n","#         losses.append(loss.item())\n","        \n","#         loss.backward()\n","#         l_optimizer.step()\n","#         l_optimizer.zero_grad()        \n","#         l_scheduler.step()\n","        \n","#     ed = time.time()\n","#     print(f'[Epoch {epoch+1}/{epoches}] Train Loss: {np.mean(losses):.2f}, time: {ed-st:.0f}s')\n","#     ## 更新质心和伪标签\n","#     outputs, km_centroids, y_true, assign_labels, pseudo_labels = clustering(model)\n","    \n","#     current_preds = pseudo_labels\n","#     evalute(outputs['y_true'], outputs['y_pred'])\n","#     ## 计算当前 两次伪标签的距离小于某个值时可以用于提前停止，目前暂未使用\n","# #     delta_label = np.sum(current_preds != last_preds).astype(np.float32)/ current_preds.shape[0]\n","# #     last_preds = np.copy(current_preds)\n","    \n","#     ## 质心引导，对比学习训练（此处输入label为伪标签）\n","#     losses2 = []\n","# #     model.train()\n","#     for i, (input_ids, attention_mask, labels) in enumerate(dataloader):\n","#         labels_ = torch.tensor(pseudo_labels[batch_size*i:batch_size*(i+1)])\n","#         labels_ = F.one_hot(labels_, num_classes=num_classes)\n","#         input_ids, attention_mask, labels_ = input_ids.to(device, dtype=torch.long), attention_mask.to(device, dtype=torch.long), labels_.to(device, dtype=torch.float)\n","#         # random\n","#         feats_a,logits_a = model(input_ids, attention_mask)\n","#         feats_b,logits_b = model(input_ids, attention_mask)\n","    \n","#         norm_feats_a = F.normalize(feats_a)\n","#         norm_feats_b = F.normalize(feats_b)\n","        \n","#         ## 计算对比学习\n","#         constrastive_feats = torch.cat((norm_feats_a.unsqueeze(1), norm_feats_b.unsqueeze(1)), dim = 1)\n","#         loss_contrast = contrast_criterion(constrastive_feats, labels = labels_.argmax(axis=1), temperature = 0.07, device = device)\n","        \n","#         ## 伪标签与预测差异loss\n","#         loss_ce = 0.5 * (criterion(logits_a, labels_) + criterion(logits_b, labels_)) \n","                    \n","#         loss = loss_contrast + loss_ce\n","\n","#         loss.backward()\n","#         optimizer.step()\n","#         optimizer.zero_grad()\n","#         losses2.append(loss.item())\n","#         scheduler.step()\n","#     ed = time.time()\n","#     print(f'[Epoch {epoch+1}/{epoches}] Train Loss: {np.mean(losses2):.2f}, time: {ed-st:.0f}s')\n","# #     # wandb\n","# #     wandb.log({\n","# #         f\"Epoch\": epoch+1,\n","# #         f\"avg_train_loss\": np.mean(losses2),\n","# #     })"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T09:21:40.017247Z","iopub.status.busy":"2024-05-30T09:21:40.016445Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[Epoch 1/200] Train Loss: 4.34, Test Loss: 4.34, Test acc: 0.02, Test evaluate: 0.23462381038353725, 0.015279024927785295, 0.020454545454545454, lr: 2.0000000000000003e-06, time: 121s\n","[Epoch 2/200] Train Loss: 4.34, Test Loss: 4.34, Test acc: 0.09, Test evaluate: 0.2799045074713729, 0.028351555228067066, 0.08636363636363636, lr: 4.000000000000001e-06, time: 122s\n","[Epoch 3/200] Train Loss: 4.34, Test Loss: 4.32, Test acc: 0.29, Test evaluate: 0.5264191273868318, 0.12845929492220845, 0.28733766233766234, lr: 6e-06, time: 122s\n","[Epoch 4/200] Train Loss: 4.25, Test Loss: 4.17, Test acc: 0.27, Test evaluate: 0.5375180474932924, 0.1477511171289806, 0.2714285714285714, lr: 8.000000000000001e-06, time: 122s\n","[Epoch 5/200] Train Loss: 4.14, Test Loss: 4.12, Test acc: 0.30, Test evaluate: 0.5571356844119775, 0.16550950396851916, 0.29967532467532465, lr: 1e-05, time: 122s\n","[Epoch 6/200] Train Loss: 4.08, Test Loss: 4.06, Test acc: 0.33, Test evaluate: 0.5724919297913799, 0.1801452527900451, 0.3181818181818182, lr: 1.2e-05, time: 122s\n","[Epoch 7/200] Train Loss: 4.03, Test Loss: 4.04, Test acc: 0.35, Test evaluate: 0.5923147355316322, 0.19233211040939155, 0.3474025974025974, lr: 1.4e-05, time: 122s\n","[Epoch 8/200] Train Loss: 3.99, Test Loss: 4.01, Test acc: 0.36, Test evaluate: 0.6209208802493092, 0.22249633409764763, 0.3587662337662338, lr: 1.6000000000000003e-05, time: 122s\n","[Epoch 9/200] Train Loss: 3.97, Test Loss: 3.98, Test acc: 0.39, Test evaluate: 0.6174017821258428, 0.2499344406067292, 0.37922077922077924, lr: 1.8e-05, time: 122s\n","[Epoch 10/200] Train Loss: 3.94, Test Loss: 3.95, Test acc: 0.42, Test evaluate: 0.65310893556595, 0.23583279329763382, 0.41883116883116883, lr: 2e-05, time: 122s\n","[Epoch 11/200] Train Loss: 3.87, Test Loss: 3.92, Test acc: 0.45, Test evaluate: 0.6605704673382717, 0.28460827167112485, 0.44545454545454544, lr: 1.9998633049924693e-05, time: 122s\n","[Epoch 12/200] Train Loss: 3.84, Test Loss: 3.88, Test acc: 0.50, Test evaluate: 0.6944095451406977, 0.293153702035495, 0.5035714285714286, lr: 1.999453257340926e-05, time: 122s\n","[Epoch 13/200] Train Loss: 3.79, Test Loss: 3.84, Test acc: 0.53, Test evaluate: 0.7113929427684862, 0.3759902542931002, 0.5256493506493507, lr: 1.998769969148305e-05, time: 122s\n","[Epoch 14/200] Train Loss: 3.76, Test Loss: 3.82, Test acc: 0.57, Test evaluate: 0.7289879200702024, 0.42129533765150734, 0.5613636363636364, lr: 1.9978136272187745e-05, time: 122s\n","[Epoch 15/200] Train Loss: 3.73, Test Loss: 3.79, Test acc: 0.58, Test evaluate: 0.7405227022632604, 0.4341226283548444, 0.5782467532467532, lr: 1.99658449300667e-05, time: 122s\n","[Epoch 16/200] Train Loss: 3.70, Test Loss: 3.76, Test acc: 0.62, Test evaluate: 0.7618311867358036, 0.4247599675301195, 0.6139610389610389, lr: 1.9950829025450116e-05, time: 122s\n","[Epoch 17/200] Train Loss: 3.66, Test Loss: 3.72, Test acc: 0.65, Test evaluate: 0.7837071725217073, 0.4765060851298553, 0.6506493506493507, lr: 1.9933092663536384e-05, time: 122s\n","[Epoch 18/200] Train Loss: 3.62, Test Loss: 3.70, Test acc: 0.67, Test evaluate: 0.7934253142110126, 0.4923959438761977, 0.6685064935064935, lr: 1.9912640693269754e-05, time: 122s\n","[Epoch 19/200] Train Loss: 3.60, Test Loss: 3.68, Test acc: 0.70, Test evaluate: 0.8045801642096235, 0.525738655682723, 0.701948051948052, lr: 1.9889478706014687e-05, time: 122s\n","[Epoch 20/200] Train Loss: 3.57, Test Loss: 3.64, Test acc: 0.74, Test evaluate: 0.8309220258772989, 0.6092640287262971, 0.7376623376623377, lr: 1.9863613034027224e-05, time: 122s\n","[Epoch 21/200] Train Loss: 3.53, Test Loss: 3.60, Test acc: 0.77, Test evaluate: 0.8511430094074993, 0.6438732265889859, 0.7756493506493507, lr: 1.9835050748723826e-05, time: 122s\n","[Epoch 22/200] Train Loss: 3.51, Test Loss: 3.59, Test acc: 0.79, Test evaluate: 0.8526728243946947, 0.6626182403188429, 0.7863636363636364, lr: 1.9803799658748096e-05, time: 122s\n","[Epoch 23/200] Train Loss: 3.49, Test Loss: 3.57, Test acc: 0.80, Test evaluate: 0.860288020924884, 0.6944711881954717, 0.8048701298701298, lr: 1.9769868307835996e-05, time: 122s\n","[Epoch 24/200] Train Loss: 3.47, Test Loss: 3.56, Test acc: 0.81, Test evaluate: 0.8641908087105858, 0.7025440617798301, 0.8107142857142857, lr: 1.973326597248006e-05, time: 122s\n","[Epoch 25/200] Train Loss: 3.46, Test Loss: 3.56, Test acc: 0.81, Test evaluate: 0.8656625151676365, 0.7047480831388155, 0.8107142857142857, lr: 1.9694002659393306e-05, time: 122s\n","[Epoch 26/200] Train Loss: 3.46, Test Loss: 3.55, Test acc: 0.81, Test evaluate: 0.8652558219116206, 0.7058865824612715, 0.8123376623376624, lr: 1.9652089102773487e-05, time: 122s\n","[Epoch 27/200] Train Loss: 3.45, Test Loss: 3.55, Test acc: 0.82, Test evaluate: 0.8656798297555178, 0.7070245284939747, 0.8146103896103896, lr: 1.9607536761368484e-05, time: 122s\n"]}],"source":["lr_list = []\n","loss_list = []\n","t_loss_list = []\n","acc_list = []\n","best_loss = 1_000\n","for epoch in range(epoches):\n","    losses = []\n","    ## train\n","    model.train()\n","    st = time.time()\n","    for input_ids, attention_mask, labels in dataloader:\n","        input_ids, attention_mask, labels = input_ids.to(device, dtype=torch.long), attention_mask.to(device, dtype=torch.long), labels.to(device, dtype=torch.float)\n","\n","        z, logits = model(input_ids, attention_mask)\n","\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()        \n","        losses.append(loss.item())\n","        scheduler.step()\n","        # 更新学习率\n","    \n","    v_losses = []\n","    v_accs = []\n","    ## validation\n","    model.eval()\n","    prs=[]\n","    gts=[]\n","    for input_ids, attention_mask, labels in val_dataloader:\n","        input_ids, attention_mask, labels = input_ids.to(device, dtype=torch.long), attention_mask.to(device, dtype=torch.long), labels.to(device, dtype=torch.float)\n","        with torch.no_grad():\n","            z,logits = model(input_ids, attention_mask)\n","            loss = criterion(logits, labels)\n","            v_losses.append(loss.item())\n","            pr = [p.argmax().cpu().numpy() for p in logits]\n","            gt = [l.argmax().cpu().numpy() for l in labels]\n","            prs.extend(pr)\n","            gts.extend(gt)\n","            acc_tmp = [p==g for p,g in zip(pr,gt)]\n","            v_accs.append(sum(acc_tmp)/len(acc_tmp))\n","    ed = time.time()\n","    lr_cur = optimizer.param_groups[0]['lr']\n","    nmi, ari, acc = evalute(gts, prs)\n","    print(f'[Epoch {epoch+1}/{epoches}] Train Loss: {np.mean(losses):.2f}, Test Loss: {np.mean(v_losses):.2f}, Test acc: {np.mean(v_accs):.2f}, Test evaluate: {nmi}, {ari}, {acc}, lr: {lr_cur}, time: {ed-st:.0f}s')\n","    # wandb\n","    wandb.log({\n","        f\"Epoch\": epoch+1,\n","        f\"avg_train_loss\": np.mean(losses),\n","        f\"avg_test_loss\": np.mean(v_losses),\n","        f\"avg_test_acc\": np.mean(v_accs),\n","        f\"nmi\": nmi,\n","        f\"ari\": ari,\n","        f\"acc\": acc,\n","        f\"lr\": lr_cur\n","    })\n","    if best_loss > np.mean(losses):\n","        best_loss = np.mean(losses)\n","        model_path = 'model_best_tr.pth'\n","        torch.save(model.state_dict(), model_path)\n","    \n","    loss_list.append(np.mean(losses))\n","    t_loss_list.append(np.mean(v_losses))\n","    acc_list.append(np.mean(v_accs))\n","    lr_list.append(lr_cur)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4969762,"sourceId":8361984,"sourceType":"datasetVersion"},{"datasetId":4996986,"sourceId":8398970,"sourceType":"datasetVersion"},{"datasetId":5108124,"isSourceIdPinned":false,"sourceId":8557534,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
